{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_train = load_dataset(\"re_dial\", revision=\"refs/convert/parquet\", split=\"train\")\n",
    "ds_test = load_dataset(\"re_dial\", revision=\"refs/convert/parquet\", split=\"test\")\n",
    "\n",
    "ds_train.to_pandas().to_parquet(\"data/raw/train.parquet\")\n",
    "ds_test.to_pandas().to_parquet(\"data/raw/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "interim_path = 'data/processed/translated_ds_011.parquet'\n",
    "\n",
    "df = pd.read_parquet(interim_path)\n",
    "print(f'total de linhas: {len(df)}')\n",
    "\n",
    "# non_translated_count = df['text_translated'].isna().sum()\n",
    "# translated_count = len(df) - non_translated_count\n",
    "\n",
    "# print(f'Total de mensagens traduzidas: {translated_count}')\n",
    "# print(f'Total de mensagens não traduzidas: {non_translated_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregando o dataset\n",
    "\n",
    "PATH = 'data/raw/ds.parquet'\n",
    "\n",
    "df = pd.read_parquet(PATH)\n",
    "\n",
    "# Dividindo o dataframe em 10 partes\n",
    "splits = np.array_split(df, 10)\n",
    "\n",
    "# Salvando cada parte como um arquivo parquet separado\n",
    "for idx, split in enumerate(splits, 1):\n",
    "    filename = f'data/raw/ds_{idx:03}.parquet'\n",
    "    split.to_parquet(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example generating batchs samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "PATH = 'data/processed'\n",
    "df_train = []\n",
    "all_movies = []\n",
    "\n",
    "df = []\n",
    "\n",
    "for filename in os.listdir(PATH):\n",
    "\n",
    "    if filename.endswith('.parquet') and filename != 'translated_ds_011.parquet':\n",
    "        df.append(pd.read_parquet(os.path.join(PATH, filename)))\n",
    "\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2text(text, movies):\n",
    "    for movie_id in re.findall(r'@\\d+', text):\n",
    "        if movies[movies['movieId'] == movie_id[1:]].empty:\n",
    "            movie_name = '<unk>'\n",
    "        else:\n",
    "            movie_name = movies[movies['movieId'] == movie_id[1:]]['movieName'].iloc[0]\n",
    "        text = text.replace(movie_id, movie_name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.DataFrame(df['movieMentions'].explode().drop_duplicates().dropna().reset_index(drop=True).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9005/9005 [01:10<00:00, 126.91it/s]\n"
     ]
    }
   ],
   "source": [
    "#from string import punctuation\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#dict_punctuation = {i: j for j, i in enumerate(punctuation)}\n",
    "\n",
    "#df_train = []\n",
    "# df = pd.read_parquet('data/processed/translated_ds_011.parquet')\n",
    "\n",
    "# for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "#     df_explode = pd.DataFrame(row[['messages_translated']].explode().tolist())\n",
    "#     # print(df_explode)\n",
    "#     df_explode['text'] = df_explode.apply(lambda x: id2text(x['text'], movies), axis=1)\n",
    "#     #print(df_explode.iloc[0])\n",
    "#     worker_id = df_explode.iloc[0]['senderWorkerId']\n",
    "#     instruction = ''\n",
    "#     response = ''\n",
    "\n",
    "#     changed = False\n",
    "\n",
    "#     for index, message in df_explode.iterrows():\n",
    "        \n",
    "#         if changed == False:\n",
    "#             if message['senderWorkerId'] == worker_id:\n",
    "#                 instruction += message['text']\n",
    "#                 if instruction[-1] not in dict_punctuation:\n",
    "#                     instruction+='.'\n",
    "#             else:\n",
    "#                 changed = True\n",
    "#                 response += message['text']\n",
    "#                 if response[-1] not in dict_punctuation:\n",
    "#                     response+='.'\n",
    "#         else:\n",
    "#             if message['senderWorkerId'] != worker_id:\n",
    "#                 response += message['text']\n",
    "#                 if response[-1] not in dict_punctuation:\n",
    "#                     response+='.'\n",
    "#             else:\n",
    "#                 changed = False\n",
    "#                 df_train.append({'initiator': instruction, 'respondant': response})\n",
    "#                 response = ''\n",
    "#                 instruction = message['text']\n",
    "#                 if instruction[-1] not in dict_punctuation:\n",
    "#                     instruction+='.'\n",
    "# df_train = pd.DataFrame(df_train)\n",
    "\n",
    "# def generate_sample(conversation):\n",
    "#         return \"<|system|>\\n Você é um chatbot de recomendação de filmes, converse com o usuário para indicar filmes apropriados.</s>\\n<|user|>\\n\" + conversation['initiator'] + \"</s>\\n<|assistant|>\\n\" + conversation['respondant'] + \"</s>\\n\"\n",
    "\n",
    "# df_train['sample'] = df_train.apply(lambda x: generate_sample(x), axis=1)\n",
    "# df_train.drop(['initiator', 'respondant'], axis=1, inplace=True)\n",
    "\n",
    "# df_train.to_parquet('data/processed/test.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nVocê é um chatbot para indicação de filmes. Responda de maneira educada sugestões de filmes para os usuários.</s>\\n<|user|>\\nHi there!</s>\\n<|assistant|>\\nNice to meet you!</s>\\n<|user|>\\nCan I ask a question?</s>\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um chatbot para indicação de filmes. Responda de maneira educada sugestões de filmes para os usuários.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi there!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1342/1342 [00:08<00:00, 166.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "\n",
    "def process_dataset(df, movies):\n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        df_explode = pd.DataFrame(row[['messages_translated']].explode().tolist())\n",
    "        df_explode['text'] = df_explode.apply(lambda x: id2text(x['text'], movies), axis=1)\n",
    "        worker_id = df_explode.iloc[0]['senderWorkerId']\n",
    "\n",
    "        message_template = [{\"role\": \"system\", \"content\": \"Você é um chatbot para indicação de filmes. Responda de maneira educada sugestões de filmes para os usuários.\"}]\n",
    "\n",
    "        for index, message in df_explode.iterrows():\n",
    "            \n",
    "            if message['senderWorkerId'] == worker_id:\n",
    "                message_template.append({\"role\": \"user\", \"content\": message['text']})\n",
    "            else:\n",
    "                message_template.append({\"role\": \"assistant\", \"content\": message['text']})\n",
    "\n",
    "        dataset.append(tokenizer.apply_chat_template(message_template, tokenize=False))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Train\n",
    "df_train = process_dataset(df, movies)\n",
    "\n",
    "# Test\n",
    "df_test = pd.read_parquet('data/processed/translated_ds_011.parquet')\n",
    "movies_test = pd.DataFrame(df_test['movieMentions'].explode().drop_duplicates().dropna().reset_index(drop=True).tolist())\n",
    "df_test = process_dataset(pd.read_parquet('data/processed/translated_ds_011.parquet'), movies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train, columns=['sample']).to_parquet('data/processed/colab/v2/train.parquet', index=False)\n",
    "pd.DataFrame(df_test, columns=['sample']).to_parquet('data/processed/colab/v2/test.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

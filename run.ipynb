{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_train = load_dataset(\"re_dial\", revision=\"refs/convert/parquet\", split=\"train\")\n",
    "ds_test = load_dataset(\"re_dial\", revision=\"refs/convert/parquet\", split=\"test\")\n",
    "\n",
    "ds_train.to_pandas().to_parquet(\"data/raw/train.parquet\")\n",
    "ds_test.to_pandas().to_parquet(\"data/raw/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "interim_path = 'data/processed/translated_ds_011.parquet'\n",
    "\n",
    "df = pd.read_parquet(interim_path)\n",
    "print(f'total de linhas: {len(df)}')\n",
    "\n",
    "# non_translated_count = df['text_translated'].isna().sum()\n",
    "# translated_count = len(df) - non_translated_count\n",
    "\n",
    "# print(f'Total de mensagens traduzidas: {translated_count}')\n",
    "# print(f'Total de mensagens não traduzidas: {non_translated_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaggregate_messages_and_translation(group):\n",
    "\n",
    "    messages = group.apply(\n",
    "        lambda x: {\n",
    "            \"timeOffset\": x['messages'][\"timeOffset\"],\n",
    "            \"text\": x['messages'][\"text\"],\n",
    "            \"senderWorkerId\": x['messages'][\"senderWorkerId\"],\n",
    "            \"messageId\": x['messages'][\"messageID\"],\n",
    "        },\n",
    "        axis=1,\n",
    "    ).tolist()\n",
    "\n",
    "    messages_translated = group.apply(\n",
    "        lambda x: {\n",
    "            \"timeOffset\": x[\"timeOffset\"],\n",
    "            \"text\": x[\"text_translated\"],\n",
    "            \"senderWorkerId\": x[\"senderWorkerId\"],\n",
    "            \"messageId\": x[\"messageID\"],\n",
    "        },\n",
    "        axis=1,\n",
    "    ).tolist()\n",
    "\n",
    "    # Colunas constantes dentro de cada grupo\n",
    "    constant_values = {\n",
    "        \"movieMentions\": group[\"movieMentions\"].iloc[0],\n",
    "        \"respondentQuestions\": group[\"respondentQuestions\"].iloc[0],\n",
    "        \"respondentWorkerId\": group[\"respondentWorkerId\"].iloc[0],\n",
    "        \"initiatorWorkerId\": group[\"initiatorWorkerId\"].iloc[0],\n",
    "        \"initiatorQuestions\": group[\"initiatorQuestions\"].iloc[0],\n",
    "    }\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"messages_translated\": messages_translated,\n",
    "            **constant_values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def reconstruct_dataset(df_messages: pd.DataFrame, number: int) -> pd.DataFrame:\n",
    "    reconstructed_df = (\n",
    "        df_messages.groupby(\"conversationId\")\n",
    "        .apply(reaggregate_messages_and_translation)\n",
    "        .reset_index()\n",
    "    )\n",
    "    reconstructed_df.to_parquet(\n",
    "        f\"data/processed/translated_ds_{number:03}.parquet\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregando o dataset\n",
    "\n",
    "PATH = 'data/raw/ds.parquet'\n",
    "\n",
    "df = pd.read_parquet(PATH)\n",
    "\n",
    "# Dividindo o dataframe em 10 partes\n",
    "splits = np.array_split(df, 10)\n",
    "\n",
    "# Salvando cada parte como um arquivo parquet separado\n",
    "for idx, split in enumerate(splits, 1):\n",
    "    filename = f'data/raw/ds_{idx:03}.parquet'\n",
    "    split.to_parquet(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example generating batchs samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split question x response explode\n",
    "\n",
    "# iterate data/processed with .parquet end\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "PATH = 'data/processed'\n",
    "df_train = []\n",
    "all_movies = []\n",
    "\n",
    "df = []\n",
    "\n",
    "for filename in os.listdir(PATH):\n",
    "\n",
    "    if filename.endswith('.parquet') and filename != 'translated_ds_011.parquet':\n",
    "        df.append(pd.read_parquet(os.path.join(PATH, filename)))\n",
    "\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2text(text, movies):\n",
    "    for movie_id in re.findall(r'@\\d+', text):\n",
    "        if movies[movies['movieId'] == movie_id[1:]].empty:\n",
    "            movie_name = '<unk>'\n",
    "        else:\n",
    "            movie_name = movies[movies['movieId'] == movie_id[1:]]['movieName'].iloc[0]\n",
    "        text = text.replace(movie_id, movie_name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.DataFrame(df['movieMentions'].explode().drop_duplicates().dropna().reset_index(drop=True).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1342 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1342/1342 [00:11<00:00, 115.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "\n",
    "dict_punctuation = {i: j for j, i in enumerate(punctuation)}\n",
    "\n",
    "df_train = []\n",
    "df = pd.read_parquet('data/processed/translated_ds_011.parquet')\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    df_explode = pd.DataFrame(row[['messages_translated']].explode().tolist())\n",
    "    #print(df_explode)\n",
    "    df_explode['text'] = df_explode.apply(lambda x: id2text(x['text'], movies), axis=1)\n",
    "    #print(df_explode.iloc[0])\n",
    "    worker_id = df_explode.iloc[0]['senderWorkerId']\n",
    "    instruction = ''\n",
    "    response = ''\n",
    "\n",
    "    changed = False\n",
    "\n",
    "    # iterate over messages in conversation\n",
    "    for index, message in df_explode.iterrows():\n",
    "        \n",
    "        if changed == False:\n",
    "            if message['senderWorkerId'] == worker_id:\n",
    "                instruction += message['text']\n",
    "                if instruction[-1] not in dict_punctuation:\n",
    "                    instruction+='.'\n",
    "            else:\n",
    "                changed = True\n",
    "                response += message['text']\n",
    "                if response[-1] not in dict_punctuation:\n",
    "                    response+='.'\n",
    "        else:\n",
    "            if message['senderWorkerId'] != worker_id:\n",
    "                response += message['text']\n",
    "                if response[-1] not in dict_punctuation:\n",
    "                    response+='.'\n",
    "            else:\n",
    "                changed = False\n",
    "                df_train.append({'initiator': instruction, 'respondant': response})\n",
    "                response = ''\n",
    "                instruction = message['text']\n",
    "                if instruction[-1] not in dict_punctuation:\n",
    "                    instruction+='.'\n",
    "\n",
    "            # df_train.append({'question': instruction, 'response': message['text']})\n",
    "            # instruction = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initiator</th>\n",
       "      <th>respondant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olá, estou procurando por um filme como o Super Troopers (2001).</td>\n",
       "      <td>Você deveria assistir Police Academy  (1984).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>É um grande? Eu nunca vi isso. Eu já vi American Pie .Eu quero dizer American Pie  (1999)</td>\n",
       "      <td>Sim, Police Academy  (1984) é muito engraçado e Police Academy 2: Their First Assignment (1985) também.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parece que eu preciso dar uma olhada neles.</td>\n",
       "      <td>Sim, você vai gostar deles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eu agradeço seu tempo. Eu precisarei dar uma olhada nisso. Existem outros que você recomendaria?</td>\n",
       "      <td>Sim Lethal Weapon (1987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obrigado, eu também vou assistir isso.</td>\n",
       "      <td>E também Beverly Hills Cop (1984)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          initiator   \n",
       "0                                  Olá, estou procurando por um filme como o Super Troopers (2001).  \\\n",
       "1         É um grande? Eu nunca vi isso. Eu já vi American Pie .Eu quero dizer American Pie  (1999)   \n",
       "2                                                       Parece que eu preciso dar uma olhada neles.   \n",
       "3  Eu agradeço seu tempo. Eu precisarei dar uma olhada nisso. Existem outros que você recomendaria?   \n",
       "4                                                            Obrigado, eu também vou assistir isso.   \n",
       "\n",
       "                                                                                                respondant  \n",
       "0                                                            Você deveria assistir Police Academy  (1984).  \n",
       "1  Sim, Police Academy  (1984) é muito engraçado e Police Academy 2: Their First Assignment (1985) também.  \n",
       "2                                                                              Sim, você vai gostar deles.  \n",
       "3                                                                                 Sim Lethal Weapon (1987)  \n",
       "4                                                                        E também Beverly Hills Cop (1984)  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(conversation):\n",
    "        return \"<|system|>\\n Você é um chatbot de recomendação de filmes, converse com o usuário para indicar filmes apropriados.</s>\\n<|user|>\\n\" + conversation['initiator'] + \"</s>\\n<|assistant|>\\n\" + conversation['respondant'] + \"</s>\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sample'] = df_train.apply(lambda x: generate_sample(x), axis=1)\n",
    "df_train.drop(['initiator', 'respondant'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet('data/processed/test.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6857, 1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\n Você é um chatbot de recomendação de filmes, converse com o usuário para indicar filmes apropriados.</s>\\n<|user|>\\nOlá, estou procurando por um filme como o Super Troopers (2001).</s>\\n<|assistant|>\\nVocê deveria assistir Police Academy  (1984).</s>\\n'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['sample'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
